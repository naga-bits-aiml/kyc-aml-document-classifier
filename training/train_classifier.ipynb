{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Document classifier training\n",
        "\n",
        "This notebook builds a TensorFlow EfficientNet-based classifier for KYC/AML document images. It expects the dataset to be organized as one folder per class (e.g., `dataset_generator/output_dataset/<class_name>/*.png`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Update the `dataset_root` path if your dataset lives elsewhere. The notebook uses TensorFlow's `image_dataset_from_directory` to split the data into training and validation subsets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "# Paths and hyperparameters\n",
        "# Point this to dataset_generator/output_dataset or your own dataset directory\n",
        "# where files are organized as dataset_root/<class_name>/*.png\n",
        "\n",
        "dataset_root = Path('dataset_generator/output_dataset')\n",
        "img_size = (224, 224)\n",
        "batch_size = 16\n",
        "epochs = 10\n",
        "validation_split = 0.2\n",
        "seed = 1337\n",
        "\n",
        "print(f\"Dataset root: {dataset_root.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the dataset\n",
        "This uses TensorFlow's `image_dataset_from_directory` to create batched datasets. Caching and prefetching keep the input pipeline fast.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure the dataset exists\n",
        "data_dir = dataset_root\n",
        "if not data_dir.exists():\n",
        "    raise FileNotFoundError(f\"Dataset directory {data_dir} does not exist. Generate it with dataset_generator or update dataset_root.\")\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=validation_split,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    data_dir,\n",
        "    validation_split=validation_split,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(f\"Classes ({num_classes}): {class_names}\")\n",
        "\n",
        "autotune = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.shuffle(1000).prefetch(autotune)\n",
        "val_ds = val_ds.cache().prefetch(autotune)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the model\n",
        "We start from ImageNet-pretrained EfficientNetB0, add light augmentation, and train only the classification head. You can unfreeze the base later for fine-tuning if needed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.05),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "], name='augmentation')\n",
        "\n",
        "preprocess = tf.keras.applications.efficientnet.preprocess_input\n",
        "\n",
        "inputs = tf.keras.Input(shape=img_size + (3,))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess(x)\n",
        "base_model = tf.keras.applications.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=x,\n",
        "    pooling='avg'\n",
        ")\n",
        "base_model.trainable = False  # start with frozen backbone\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.3)(base_model.output)\n",
        "outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train\n",
        "Early stopping and model checkpointing prevent overfitting and keep the best weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss'),\n",
        "    tf.keras.callbacks.ModelCheckpoint('training/model/efficientnet_document_classifier.keras',\n",
        "                                      monitor='val_accuracy',\n",
        "                                      save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Fine-tune the base network\n",
        "Unfreeze the top layers of the backbone for a few more epochs if you need extra accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fine_tune_at = 200  # unfreeze the last blocks\n",
        "\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate and export\n",
        "Save the best-performing model and a label map for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_results = model.evaluate(val_ds)\n",
        "print(f\"Validation loss: {eval_results[0]:.4f} - acc: {eval_results[1]:.4f}\")\n",
        "\n",
        "export_dir = Path('training/model')\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model_path = export_dir / 'efficientnet_document_classifier.keras'\n",
        "labels_path = export_dir / 'labels.txt'\n",
        "\n",
        "model.save(model_path)\n",
        "labels_path.write_text('\n",
        "'.join(class_names))\n",
        "\n",
        "print(f\"Saved model to {model_path}\")\n",
        "print(f\"Saved label map to {labels_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}