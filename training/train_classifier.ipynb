{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cdfafc1",
   "metadata": {},
   "source": [
    "# Train KYC/AML Document Classifier\n",
    "\n",
    "This notebook trains an **EfficientNetB0** model to classify 5 types of ID documents:\n",
    "- Aadhaar Card\n",
    "- Driving License\n",
    "- PAN Card\n",
    "- Voter ID\n",
    "- Passport\n",
    "\n",
    "The trained model will be saved to `training/model/` for use by the inference microservice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88aef9",
   "metadata": {},
   "source": [
    "## Step 0: Install Required Dependencies\n",
    "\n",
    "Before running this notebook, make sure you have activated the conda environment and installed all required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62941a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version   \n",
    "!hostname\n",
    "!uname -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d773a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow 2.16.1 fully supports Python 3.9‚Äì3.12 and TPUs\n",
    "%pip install tensorflow==2.16.1 pillow numpy==1.26.4 matplotlib scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a269b211",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6690551",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Avoid serialization issues with TensorBoard\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom callback to convert EagerTensor/log arrays into JSON-safe floats\n",
    "class ConvertHistoryCallback(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if not logs:\n",
    "            return\n",
    "\n",
    "        for key, value in logs.items():\n",
    "            if hasattr(value, \"numpy\"):\n",
    "                value = value.numpy()\n",
    "\n",
    "            if isinstance(value, (np.ndarray, np.generic)):\n",
    "                if value.shape == ():\n",
    "                    logs[key] = float(value)\n",
    "                else:\n",
    "                    logs[key] = [float(v) for v in value.flatten().tolist()]\n",
    "            elif isinstance(value, (list, tuple)):\n",
    "                logs[key] = [float(v) for v in np.asarray(value).flatten().tolist()]\n",
    "            else:\n",
    "                try:\n",
    "                    logs[key] = float(value)\n",
    "                except (TypeError, ValueError):\n",
    "                    logs[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbcde16",
   "metadata": {},
   "source": [
    "## Step 2: Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bafc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "TRAIN_DIR = \"../dataset_generator/dataset/train\"\n",
    "VALID_DIR = \"../dataset_generator/dataset/valid\"\n",
    "\n",
    "# Training parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Model output\n",
    "MODEL_DIR = \"model\"\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"efficientnet_model.h5\")\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Train directory: {TRAIN_DIR}\")\n",
    "print(f\"Valid directory: {VALID_DIR}\")\n",
    "print(f\"Image size: {IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Model will be saved to: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1ca3f",
   "metadata": {},
   "source": [
    "## Step 3: Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images_in_dataset(base_dir):\n",
    "    \"\"\"Count images per class\"\"\"\n",
    "    stats = {}\n",
    "    \n",
    "    if not os.path.exists(base_dir):\n",
    "        print(f\"‚ùå Directory not found: {base_dir}\")\n",
    "        return stats\n",
    "    \n",
    "    for class_name in os.listdir(base_dir):\n",
    "        class_dir = os.path.join(base_dir, class_name)\n",
    "        if os.path.isdir(class_dir):\n",
    "            image_files = [f for f in os.listdir(class_dir) \n",
    "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            stats[class_name] = len(image_files)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Count images\n",
    "train_stats = count_images_in_dataset(TRAIN_DIR)\n",
    "valid_stats = count_images_in_dataset(VALID_DIR)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä DATASET STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nTRAIN SET:\")\n",
    "train_total = 0\n",
    "for class_name, count in sorted(train_stats.items()):\n",
    "    print(f\"  - {class_name}: {count} images\")\n",
    "    train_total += count\n",
    "print(f\"  TOTAL: {train_total} images\")\n",
    "\n",
    "print(\"\\nVALIDATION SET:\")\n",
    "valid_total = 0\n",
    "for class_name, count in sorted(valid_stats.items()):\n",
    "    print(f\"  - {class_name}: {count} images\")\n",
    "    valid_total += count\n",
    "print(f\"  TOTAL: {valid_total} images\")\n",
    "\n",
    "print(f\"\\nüéØ GRAND TOTAL: {train_total + valid_total} images\")\n",
    "print(f\"üìã Number of classes: {len(train_stats)}\")\n",
    "\n",
    "if len(train_stats) == 5 and len(valid_stats) == 5:\n",
    "    print(\"\\n‚úÖ All 5 classes present in both splits!\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Expected 5 classes, found {len(train_stats)} in train, {len(valid_stats)} in valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1497b57",
   "metadata": {},
   "source": [
    "## Step 4: Create Data Generators with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09769e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data generator with augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation data generator (only rescaling, no augmentation)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    VALID_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Print class indices\n",
    "print(\"\\nüìã Class Indices:\")\n",
    "for class_name, idx in sorted(train_generator.class_indices.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx}: {class_name}\")\n",
    "\n",
    "NUM_CLASSES = len(train_generator.class_indices)\n",
    "print(f\"\\n‚úì Data generators created\")\n",
    "print(f\"  Train samples: {train_generator.samples}\")\n",
    "print(f\"  Valid samples: {valid_generator.samples}\")\n",
    "print(f\"  Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc1e57",
   "metadata": {},
   "source": [
    "## Step 5: Display Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014c23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "sample_batch, sample_labels = next(train_generator)\n",
    "\n",
    "# Display 9 sample images\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Reverse class indices for display\n",
    "class_names = {v: k for k, v in train_generator.class_indices.items()}\n",
    "\n",
    "for i in range(9):\n",
    "    img = sample_batch[i]\n",
    "    label_idx = np.argmax(sample_labels[i])\n",
    "    class_name = class_names[label_idx]\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'{class_name}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Sample Training Images (with augmentation)', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reset generator\n",
    "train_generator.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657b788d",
   "metadata": {},
   "source": [
    "## Step 6: Build EfficientNetB0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469829fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes, img_size=(224, 224)):\n",
    "    \"\"\"Build EfficientNetB0 model with custom top layers\"\"\"\n",
    "    \n",
    "    # Load pre-trained EfficientNetB0 (without top classification layer)\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(*img_size, 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    inputs = keras.Input(shape=(*img_size, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_model(NUM_CLASSES, IMG_SIZE)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Model built and compiled\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c871b",
   "metadata": {},
   "source": [
    "## Step 7: Configure Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to convert metrics\n",
    "convert_callback = ConvertHistoryCallback()\n",
    "\n",
    "# Model checkpoint - save best model\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    MODEL_PATH,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1,\n",
    "    save_weights_only=False\n",
    ")\n",
    "\n",
    "# Early stopping - stop if no improvement\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Reduce learning rate when metric plateaus\n",
    "reduce_lr_callback = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    convert_callback,\n",
    "    checkpoint_callback,\n",
    "    early_stopping_callback,\n",
    "    reduce_lr_callback\n",
    "]\n",
    "\n",
    "print(\"‚úì Callbacks configured:\")\n",
    "print(\"  - ConvertHistoryCallback: Convert EagerTensor to float\")\n",
    "print(\"  - ModelCheckpoint: Save best model based on val_accuracy\")\n",
    "print(\"  - EarlyStopping: Stop if no improvement for 5 epochs\")\n",
    "print(\"  - ReduceLROnPlateau: Reduce LR if val_loss plateaus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f9c249",
   "metadata": {},
   "source": [
    "## Step 8: Train Model (Phase 1 - Frozen Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc965c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PHASE 1: Training with frozen base model\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training for {EPOCHS // 2} epochs...\\n\")\n",
    "\n",
    "# Train with frozen base (metrics will be auto-converted by callback)\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=EPOCHS // 2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 1 training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1bde9",
   "metadata": {},
   "source": [
    "## Step 9: Fine-tune Model (Phase 2 - Unfrozen Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631708c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ PHASE 2: Fine-tuning with unfrozen base model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze early layers, unfreeze later layers\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')]\n",
    ")\n",
    "\n",
    "print(f\"‚úì Base model unfrozen (layers 100+ trainable)\")\n",
    "print(f\"‚úì Learning rate reduced to {LEARNING_RATE / 10}\")\n",
    "print(f\"\\nTraining for {EPOCHS - (EPOCHS // 2)} more epochs...\\n\")\n",
    "\n",
    "# Continue training with unfrozen base\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=EPOCHS - (EPOCHS // 2),\n",
    "    initial_epoch=len(history_phase1.history['loss']),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2 fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638de54b",
   "metadata": {},
   "source": [
    "## Step 10: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories and convert to standard Python types\n",
    "history = {}\n",
    "\n",
    "def _to_float_list(values):\n",
    "    \"\"\"Convert tensors/arrays coming from TF history into plain floats.\"\"\"\n",
    "    converted = []\n",
    "    for value in values:\n",
    "        if hasattr(value, 'numpy'):\n",
    "            value = value.numpy()\n",
    "        if isinstance(value, (list, tuple)):\n",
    "            value = np.asarray(value)\n",
    "        if isinstance(value, np.ndarray):\n",
    "            if value.size == 1:\n",
    "                value = value.item()\n",
    "            else:\n",
    "                # Collapse unexpected vector outputs by averaging to a scalar\n",
    "                value = float(np.mean(value))\n",
    "        converted.append(float(value))\n",
    "    return converted\n",
    "\n",
    "for key in history_phase1.history.keys():\n",
    "    phase1_values = _to_float_list(history_phase1.history[key])\n",
    "    phase2_values = _to_float_list(history_phase2.history[key])\n",
    "    history[key] = phase1_values + phase2_values\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot accuracy\n",
    "axes[0, 0].plot(history['accuracy'], label='Train Accuracy')\n",
    "axes[0, 0].plot(history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0, 0].axvline(x=len(history_phase1.history['loss']), color='r', linestyle='--', label='Fine-tuning start')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Model Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Plot loss\n",
    "axes[0, 1].plot(history['loss'], label='Train Loss')\n",
    "axes[0, 1].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0, 1].axvline(x=len(history_phase1.history['loss']), color='r', linestyle='--', label='Fine-tuning start')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].set_title('Model Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Plot top-2 accuracy\n",
    "axes[1, 0].plot(history['top2_accuracy'], label='Train Top-2 Acc')\n",
    "axes[1, 0].plot(history['val_top2_accuracy'], label='Val Top-2 Acc')\n",
    "axes[1, 0].axvline(x=len(history_phase1.history['loss']), color='r', linestyle='--', label='Fine-tuning start')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Top-2 Accuracy')\n",
    "axes[1, 0].set_title('Top-2 Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Summary text\n",
    "final_train_acc = history['accuracy'][-1]\n",
    "final_val_acc = history['val_accuracy'][-1]\n",
    "best_val_acc = max(history['val_accuracy'])\n",
    "summary_text = f\"\"\"Final Metrics:\n",
    "\n",
    "Train Accuracy: {final_train_acc:.4f}\n",
    "Val Accuracy: {final_val_acc:.4f}\n",
    "Best Val Accuracy: {best_val_acc:.4f}\n",
    "\n",
    "Total Epochs: {len(history['loss'])}\n",
    "Phase 1: {len(history_phase1.history['loss'])} epochs\n",
    "Phase 2: {len(history_phase2.history['loss'])} epochs\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, summary_text, fontsize=12, verticalalignment='center')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.suptitle('Training History', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"üìä Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"üìä Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e4ef2",
   "metadata": {},
   "source": [
    "## Step 11: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4da93",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATING MODEL ON VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate on validation set\n",
    "results = model.evaluate(valid_generator, verbose=1)\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Loss: {results[0]:.4f}\")\n",
    "print(f\"  Accuracy: {results[1]:.4f}\")\n",
    "print(f\"  Top-2 Accuracy: {results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09a328",
   "metadata": {},
   "source": [
    "## Step 12: Generate Predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44533708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Generate predictions\n",
    "print(\"Generating predictions...\")\n",
    "valid_generator.reset()\n",
    "predictions = model.predict(valid_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Get true labels\n",
    "true_classes = valid_generator.classes\n",
    "class_labels = list(valid_generator.class_indices.keys())\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(true_classes, predicted_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3e327c",
   "metadata": {},
   "source": [
    "## Step 13: Test Model on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c27fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of validation images\n",
    "valid_generator.reset()\n",
    "sample_batch, sample_labels = next(valid_generator)\n",
    "\n",
    "# Make predictions\n",
    "sample_predictions = model.predict(sample_batch)\n",
    "\n",
    "# Display 9 sample predictions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "class_names = {v: k for k, v in valid_generator.class_indices.items()}\n",
    "\n",
    "for i in range(9):\n",
    "    img = sample_batch[i]\n",
    "    true_label_idx = np.argmax(sample_labels[i])\n",
    "    pred_label_idx = np.argmax(sample_predictions[i])\n",
    "    confidence = sample_predictions[i][pred_label_idx]\n",
    "    \n",
    "    true_label = class_names[true_label_idx]\n",
    "    pred_label = class_names[pred_label_idx]\n",
    "    \n",
    "    # Color: green if correct, red if wrong\n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    \n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2%}',\n",
    "                     fontsize=10, color=color, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1abf0",
   "metadata": {},
   "source": [
    "## Step 14: Save Model and Class Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c6d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save model (already saved by checkpoint, but save final version)\n",
    "final_model_path = os.path.join(MODEL_DIR, \"efficientnet_model_final.h5\")\n",
    "model.save(final_model_path)\n",
    "print(f\"‚úÖ Final model saved to: {final_model_path}\")\n",
    "\n",
    "# Save class indices for inference\n",
    "class_indices_path = os.path.join(MODEL_DIR, \"class_indices.json\")\n",
    "with open(class_indices_path, 'w') as f:\n",
    "    json.dump(train_generator.class_indices, f, indent=2)\n",
    "print(f\"‚úÖ Class indices saved to: {class_indices_path}\")\n",
    "\n",
    "# Save model architecture as JSON\n",
    "model_json_path = os.path.join(MODEL_DIR, \"model_architecture.json\")\n",
    "with open(model_json_path, 'w') as f:\n",
    "    f.write(model.to_json())\n",
    "print(f\"‚úÖ Model architecture saved to: {model_json_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel files saved in: {os.path.abspath(MODEL_DIR)}\")\n",
    "print(f\"  - {os.path.basename(MODEL_PATH)} (best model)\")\n",
    "print(f\"  - {os.path.basename(final_model_path)} (final model)\")\n",
    "print(f\"  - {os.path.basename(class_indices_path)} (class mapping)\")\n",
    "print(f\"  - {os.path.basename(model_json_path)} (architecture)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba320000",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ Model trained on 5 document classes  \n",
    "‚úÖ Two-phase training: frozen base ‚Üí fine-tuned base  \n",
    "‚úÖ Best model saved based on validation accuracy  \n",
    "‚úÖ Model ready for inference microservice  \n",
    "\n",
    "**Model Files:**\n",
    "```\n",
    "training/model/\n",
    "‚îú‚îÄ‚îÄ efficientnet_model.h5          ‚Üê Best model (use this for inference)\n",
    "‚îú‚îÄ‚îÄ efficientnet_model_final.h5    ‚Üê Final epoch model\n",
    "‚îú‚îÄ‚îÄ class_indices.json              ‚Üê Class name to index mapping\n",
    "‚îî‚îÄ‚îÄ model_architecture.json         ‚Üê Model architecture (optional)\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "1. Load the model in the inference microservice\n",
    "2. Test the API: `uvicorn api.main:app --reload --port 8000`\n",
    "3. Deploy the microservice using Docker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
