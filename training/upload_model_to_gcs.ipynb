{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4f339e",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5811132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-storage in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.26.1 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-cloud-storage) (2.43.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=2.27.0 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-cloud-storage) (2.28.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-cloud-storage) (2.5.0)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.7.2 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-cloud-storage) (2.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.22.0 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-cloud-storage) (2.32.5)\n",
      "Requirement already satisfied: google-crc32c<2.0.0,>=1.1.3 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-cloud-storage) (1.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.72.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (4.25.8)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-api-core<3.0.0,>=2.27.0->google-cloud-storage) (1.26.1)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (6.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from google-auth<3.0.0,>=2.26.1->google-cloud-storage) (4.9.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from requests<3.0.0,>=2.22.0->google-cloud-storage) (2025.11.12)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\lenovo\\anaconda3\\envs\\kyc-aml-env\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.26.1->google-cloud-storage) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4203da",
   "metadata": {},
   "source": [
    "## Step 2: Configure GCS Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2b6205a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model files to upload:\n",
      "\n",
      "\tefficientnet_model.pth\n",
      "\n",
      "\tclass_indices.json\n",
      "\n",
      "\ttraining_history.json\n",
      "\n",
      "\tefficientnet_model.onnx\n",
      "\n",
      "Bucket: kyc-aml-model\n",
      "Will upload to: gs://kyc-aml-model/document_classification/v1/\n",
      "Local model directory: model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# GCS Configuration\n",
    "PROJECT_ID = \"kyc-aml-automation\"  # Replace with your GCP project ID\n",
    "BUCKET_NAME = \"kyc-aml-model\"  # Bucket for trained models\n",
    "SERVICE_ACCOUNT_KEY = \"C:\\\\Users\\\\Lenovo\\\\.ssh\\\\gcp\\\\service_account.json\"  # Replace with path to your key file\n",
    "\n",
    "# Local model directory\n",
    "MODEL_DIR = \"model\"\n",
    "\n",
    "# Required model files (must exist)\n",
    "REQUIRED_FILES = [\n",
    "    \"efficientnet_model.pth\",      # PyTorch checkpoint\n",
    "    \"class_indices.json\",          # Class mappings\n",
    "    \"training_history.json\"        # Training metrics\n",
    "]\n",
    "\n",
    "# Optional model files (upload if available)\n",
    "OPTIONAL_FILES = [\n",
    "    \"efficientnet_model.onnx\",     # ONNX export (optional)\n",
    "]\n",
    "\n",
    "# All model files to check\n",
    "MODEL_FILES = REQUIRED_FILES + OPTIONAL_FILES\n",
    "\n",
    "# GCS paths - organized by model type and version\n",
    "\n",
    "GCS_MODEL_PREFIX = \"document_classification/v1\"  # identity/version structure    print(f\"  - {file}\")\n",
    "\n",
    "print(f\"\\nModel files to upload:\")\n",
    "for file in MODEL_FILES:\n",
    "    print(f\"\\n\\t{file}\")\n",
    "print(\"\")    \n",
    "\n",
    "print(f\"Bucket: {BUCKET_NAME}\")\n",
    "print(f\"Will upload to: gs://{BUCKET_NAME}/{GCS_MODEL_PREFIX}/\")\n",
    "print(f\"Local model directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ac3f7",
   "metadata": {},
   "source": [
    "## Step 3: Verify Model Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e495bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_model_files(model_dir, required_files, optional_files):\n",
    "    \"\"\"Verify that required model files exist and check for optional files\"\"\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"‚ùå Model directory not found: {model_dir}\")\n",
    "        return False, []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä MODEL FILES VERIFICATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_required_found = True\n",
    "    total_size = 0\n",
    "    found_files = []\n",
    "    \n",
    "    print(\"\\nRequired Files:\")\n",
    "    for file_name in required_files:\n",
    "        file_path = os.path.join(model_dir, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "            found_files.append(file_name)\n",
    "            print(f\"  ‚úì {file_name}: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {file_name}: NOT FOUND (REQUIRED)\")\n",
    "            all_required_found = False\n",
    "    \n",
    "    print(\"\\nOptional Files:\")\n",
    "    for file_name in optional_files:\n",
    "        file_path = os.path.join(model_dir, file_name)\n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "            found_files.append(file_name)\n",
    "            print(f\"  ‚úì {file_name}: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  {file_name}: NOT FOUND (optional - will skip)\")\n",
    "    \n",
    "    print(f\"\\n  TOTAL SIZE: {total_size / (1024*1024):.2f} MB\")\n",
    "    print(f\"  FILES TO UPLOAD: {len(found_files)}/{len(required_files + optional_files)}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return all_required_found, found_files\n",
    "\n",
    "# Verify model files\n",
    "all_required_found, files_to_upload = verify_model_files(MODEL_DIR, REQUIRED_FILES, OPTIONAL_FILES)\n",
    "\n",
    "if all_required_found:\n",
    "    print(f\"\\n‚úÖ All required model files found! ({len(files_to_upload)} files will be uploaded)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Some required model files are missing!\")\n",
    "    print(\"Please train the model first (train_classifier.ipynb)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36cee0",
   "metadata": {},
   "source": [
    "## Step 4: Authenticate with Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09609cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Authenticated with GCP project: kyc-aml-automation\n",
      "‚úÖ Found 2 bucket(s)\n",
      "‚úÖ Found 2 bucket(s)\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Set credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = SERVICE_ACCOUNT_KEY\n",
    "\n",
    "# Initialize GCS client\n",
    "try:\n",
    "    client = storage.Client(project=PROJECT_ID)\n",
    "    print(f\"‚úÖ Authenticated with GCP project: {PROJECT_ID}\")\n",
    "    \n",
    "    # List buckets to verify access\n",
    "    buckets = list(client.list_buckets())\n",
    "    print(f\"‚úÖ Found {len(buckets)} bucket(s)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Authentication failed: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Verify SERVICE_ACCOUNT_KEY path is correct\")\n",
    "    print(\"2. Ensure service account has 'Storage Admin' role\")\n",
    "    print(\"3. Check PROJECT_ID is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fec50a",
   "metadata": {},
   "source": [
    "## Step 5: Create or Verify Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a399d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found existing bucket: kyc-aml-model\n",
      "\n",
      "Bucket info:\n",
      "  Name: kyc-aml-model\n",
      "  Location: US-CENTRAL1\n",
      "  Storage class: STANDARD\n"
     ]
    }
   ],
   "source": [
    "def get_or_create_bucket(client, bucket_name, location='us-central1'):\n",
    "    \"\"\"Get existing bucket or create new one\"\"\"\n",
    "    try:\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        print(f\"‚úÖ Found existing bucket: {bucket_name}\")\n",
    "        return bucket\n",
    "    except Exception:\n",
    "        print(f\"üì¶ Creating new bucket: {bucket_name}\")\n",
    "        bucket = client.create_bucket(bucket_name, location=location)\n",
    "        print(f\"‚úÖ Bucket created: {bucket_name}\")\n",
    "        return bucket\n",
    "\n",
    "# Get or create bucket\n",
    "bucket = get_or_create_bucket(client, BUCKET_NAME)\n",
    "print(f\"\\nBucket info:\")\n",
    "print(f\"  Name: {bucket.name}\")\n",
    "print(f\"  Location: {bucket.location}\")\n",
    "print(f\"  Storage class: {bucket.storage_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb73ae25",
   "metadata": {},
   "source": [
    "## Step 6: Upload Model Files to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dcc00e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files_to_upload' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uploaded_blobs\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Upload only the files that were found during verification\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m uploaded_blobs \u001b[38;5;241m=\u001b[39m upload_model_files(bucket, MODEL_DIR, \u001b[43mfiles_to_upload\u001b[49m, GCS_MODEL_PREFIX)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'files_to_upload' is not defined"
     ]
    }
   ],
   "source": [
    "def upload_file_to_gcs(bucket, source_file, destination_blob_name):\n",
    "    \"\"\"Upload a single file to GCS with progress tracking\"\"\"\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    print(f\"\\n‚òÅÔ∏è  Uploading: {os.path.basename(source_file)}\")\n",
    "    print(f\"   Source: {source_file}\")\n",
    "    print(f\"   Destination: gs://{bucket.name}/{destination_blob_name}\")\n",
    "    \n",
    "    # Upload file\n",
    "    blob.upload_from_filename(source_file)\n",
    "    \n",
    "    # Get uploaded file info\n",
    "    blob.reload()\n",
    "    size_mb = blob.size / (1024 * 1024)\n",
    "    \n",
    "    print(f\"   ‚úÖ Upload complete! Size: {size_mb:.2f} MB\")\n",
    "    \n",
    "    return blob\n",
    "\n",
    "def upload_model_files(bucket, model_dir, file_list, gcs_prefix):\n",
    "    \"\"\"Upload all model files to GCS\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚òÅÔ∏è  UPLOADING MODEL FILES TO GCS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    uploaded_blobs = []\n",
    "    total_size = 0\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        source_path = os.path.join(model_dir, file_name)\n",
    "        if os.path.exists(source_path):\n",
    "            destination_path = f\"{gcs_prefix}/{file_name}\"\n",
    "            blob = upload_file_to_gcs(bucket, source_path, destination_path)\n",
    "            uploaded_blobs.append(blob)\n",
    "            total_size += blob.size\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Skipping {file_name} (not found)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"‚úÖ Upload complete! {len(uploaded_blobs)}/{len(file_list)} files uploaded\")\n",
    "    print(f\"   Total size: {total_size / (1024*1024):.2f} MB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return uploaded_blobs\n",
    "\n",
    "# Upload only the files that were found during verification\n",
    "uploaded_blobs = upload_model_files(bucket, MODEL_DIR, files_to_upload, GCS_MODEL_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a3b01",
   "metadata": {},
   "source": [
    "## Step 7: Verify Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Verifying uploads...\n",
      "============================================================\n",
      "‚úÖ efficientnet_model.pth\n",
      "   Size: 50.12 MB\n",
      "   MD5: cZVz+NSPr7qqWhKTHRKH0w==\n",
      "   GCS URI: gs://kyc-aml-model/identity_classification/v1/efficientnet_model.pth\n",
      "\n",
      "‚úÖ efficientnet_model.pth\n",
      "   Size: 50.12 MB\n",
      "   MD5: cZVz+NSPr7qqWhKTHRKH0w==\n",
      "   GCS URI: gs://kyc-aml-model/identity_classification/v1/efficientnet_model.pth\n",
      "\n",
      "‚ùå efficientnet_model.onnx: Verification failed - 404 GET https://storage.googleapis.com/storage/v1/b/kyc-aml-model/o/identity_classification%2Fv1%2Fefficientnet_model.onnx?projection=noAcl&prettyPrint=false: No such object: kyc-aml-model/identity_classification/v1/efficientnet_model.onnx\n",
      "‚ùå efficientnet_model.onnx: Verification failed - 404 GET https://storage.googleapis.com/storage/v1/b/kyc-aml-model/o/identity_classification%2Fv1%2Fefficientnet_model.onnx?projection=noAcl&prettyPrint=false: No such object: kyc-aml-model/identity_classification/v1/efficientnet_model.onnx\n",
      "‚úÖ class_indices.json\n",
      "   Size: 0.00 MB\n",
      "   MD5: bU1mWrzt3M7Rj85nuWEHNg==\n",
      "   GCS URI: gs://kyc-aml-model/identity_classification/v1/class_indices.json\n",
      "\n",
      "‚úÖ class_indices.json\n",
      "   Size: 0.00 MB\n",
      "   MD5: bU1mWrzt3M7Rj85nuWEHNg==\n",
      "   GCS URI: gs://kyc-aml-model/identity_classification/v1/class_indices.json\n",
      "\n",
      "‚úÖ training_history.json\n",
      "   Size: 0.00 MB\n",
      "   MD5: xp20G2NkLJKBv1jSl2ZeRg==\n",
      "   GCS URI: gs://kyc-aml-model/identity_classification/v1/training_history.json\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è  Some files failed verification\n",
      "‚úÖ training_history.json\n",
      "   Size: 0.00 MB\n",
      "   MD5: xp20G2NkLJKBv1jSl2ZeRg==\n",
      "   GCS URI: gs://kyc-aml-model/identity_classification/v1/training_history.json\n",
      "\n",
      "============================================================\n",
      "‚ö†Ô∏è  Some files failed verification\n"
     ]
    }
   ],
   "source": [
    "def verify_gcs_upload(bucket, gcs_prefix, file_list):\n",
    "    \"\"\"Verify uploaded files exist and are accessible\"\"\"\n",
    "    print(\"\\nüîç Verifying uploads...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    all_verified = True\n",
    "    verified_count = 0\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        blob_path = f\"{gcs_prefix}/{file_name}\"\n",
    "        try:\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.reload()\n",
    "            \n",
    "            size_mb = blob.size / (1024*1024)\n",
    "            print(f\"‚úÖ {file_name}\")\n",
    "            print(f\"   Size: {size_mb:.2f} MB\")\n",
    "            print(f\"   MD5: {blob.md5_hash}\")\n",
    "            print(f\"   GCS URI: gs://{bucket.name}/{blob.name}\")\n",
    "            print()\n",
    "            \n",
    "            verified_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {file_name}: Verification failed - {e}\")\n",
    "            all_verified = False\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Verified: {verified_count}/{len(file_list)} files\")\n",
    "    return all_verified\n",
    "\n",
    "if verify_gcs_upload(bucket, GCS_MODEL_PREFIX, files_to_upload):\n",
    "    print(\"‚úÖ All uploaded files verified successfully!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Some files failed verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1637d",
   "metadata": {},
   "source": [
    "## Step 8: Generate Signed URLs for Model Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîó GENERATING SIGNED URLS FOR MODEL FILES\n",
      "============================================================\n",
      "\n",
      "‚ùå Failed to generate signed URLs: Max allowed expiration interval is seven days 604800\n",
      "\n",
      "Note: Make sure your service account has 'Service Account Token Creator' role\n",
      "or use 'iam.serviceAccounts.signBlob' permission.\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def generate_signed_url(bucket, blob_path, expiration_days=365):\n",
    "    \"\"\"\n",
    "    Generate a signed URL that allows public download without authentication.\n",
    "    The URL expires after the specified number of days.\n",
    "    \"\"\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "    \n",
    "    # Generate signed URL (valid for specified days)\n",
    "    url = blob.generate_signed_url(\n",
    "        version=\"v4\",\n",
    "        expiration=timedelta(days=expiration_days),\n",
    "        method=\"GET\"\n",
    "    )\n",
    "    \n",
    "    return url\n",
    "\n",
    "# Generate signed URLs for all model files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîó GENERATING SIGNED URLS FOR MODEL FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "signed_urls = {}\n",
    "\n",
    "try:\n",
    "    # Only generate URLs for files that were actually uploaded\n",
    "    for file_name in files_to_upload:\n",
    "        blob_path = f\"{GCS_MODEL_PREFIX}/{file_name}\"\n",
    "        \n",
    "        url = generate_signed_url(bucket, blob_path, expiration_days=365)\n",
    "        signed_urls[file_name] = url\n",
    "        print(f\"\\n‚úÖ {file_name}\")\n",
    "        print(f\"   URL: {url[:80]}...\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Generated {len(signed_urls)} signed URLs (valid for 365 days)\")\n",
    "    \n",
    "    # Save URLs to file\n",
    "    url_file = \"model_download_urls.txt\"\n",
    "    with open(url_file, 'w') as f:\n",
    "        f.write(f\"# KYC/AML Identity Classifier - Model Download URLs\\n\")\n",
    "        f.write(f\"# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"# Valid for: 365 days\\n\")\n",
    "        f.write(f\"# GCS Bucket: gs://{BUCKET_NAME}/{GCS_MODEL_PREFIX}/\\n\\n\")\n",
    "        \n",
    "        for file_name, url in signed_urls.items():\n",
    "            f.write(f\"\\n# {file_name}\\n\")\n",
    "            f.write(f\"{url}\\n\")\n",
    "        \n",
    "        f.write(f\"\\n\\n# Usage in Python:\\n\")\n",
    "        f.write(f\"import urllib.request\\n\")\n",
    "        f.write(f\"\\n\")\n",
    "        for file_name, url in signed_urls.items():\n",
    "            f.write(f\"urllib.request.urlretrieve('{url}', '{file_name}')\\n\")\n",
    "        \n",
    "        f.write(f\"\\n\\n# Usage with wget:\\n\")\n",
    "        for file_name, url in signed_urls.items():\n",
    "            f.write(f\"wget -O {file_name} '{url}'\\n\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ URLs saved to: {url_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Failed to generate signed URLs: {e}\")\n",
    "    print(\"\\nNote: Make sure your service account has 'Service Account Token Creator' role\")\n",
    "    print(\"or use 'iam.serviceAccounts.signBlob' permission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325b88d8",
   "metadata": {},
   "source": [
    "## Step 9: Generate Deployment Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5148ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìã MODEL DEPLOYMENT INSTRUCTIONS\n",
      "============================================================\n",
      "\n",
      "# ============================================================\n",
      "# KYC/AML Identity Classifier - Model Deployment\n",
      "# ============================================================\n",
      "\n",
      "# Method 1: Download using Signed URLs (No Authentication Required)\n",
      "# Valid for 365 days from generation date\n",
      "\n",
      "import urllib.request\n",
      "import os\n",
      "\n",
      "# Create model directory\n",
      "os.makedirs('model', exist_ok=True)\n",
      "\n",
      "# Download model files\n",
      "model_urls = {\n",
      "}\n",
      "\n",
      "for file_name, url in model_urls.items():\n",
      "    print(f\"Downloading {file_name}...\")\n",
      "    urllib.request.urlretrieve(url, f\"model/{file_name}\")\n",
      "    print(f\"‚úÖ Downloaded {file_name}\")\n",
      "\n",
      "print(\"\n",
      "‚úÖ All model files downloaded!\")\n",
      "\n",
      "# ============================================================\n",
      "# Method 2: Download using GCS API (Requires Authentication)\n",
      "# ============================================================\n",
      "\n",
      "from google.cloud import storage\n",
      "import os\n",
      "\n",
      "# Set credentials (if not running on GCP)\n",
      "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/service-account.json'\n",
      "\n",
      "# Initialize client\n",
      "client = storage.Client(project='kyc-aml-automation')\n",
      "bucket = client.bucket('kyc-aml-model')\n",
      "\n",
      "# Download model files\n",
      "model_files = ['efficientnet_model.pth', 'efficientnet_model.onnx', 'class_indices.json', 'training_history.json']\n",
      "gcs_prefix = 'identity_classification/v1'\n",
      "\n",
      "os.makedirs('model', exist_ok=True)\n",
      "\n",
      "for file_name in model_files:\n",
      "    blob_path = f\"{gcs_prefix}/{file_name}\"\n",
      "    local_path = f\"model/{file_name}\"\n",
      "    \n",
      "    print(f\"Downloading {file_name}...\")\n",
      "    blob = bucket.blob(blob_path)\n",
      "    blob.download_to_filename(local_path)\n",
      "    print(f\"‚úÖ Downloaded {file_name}\")\n",
      "\n",
      "print(\"\n",
      "‚úÖ All model files downloaded!\")\n",
      "\n",
      "# ============================================================\n",
      "# Method 3: Docker Deployment (Download during build)\n",
      "# ============================================================\n",
      "\n",
      "# Add to Dockerfile:\n",
      "\n",
      "# Download models during Docker build\n",
      "RUN mkdir -p /app/model && \\\n",
      "# ============================================================\n",
      "# Method 4: Load Model in Inference Code\n",
      "# ============================================================\n",
      "\n",
      "import torch\n",
      "import json\n",
      "from torchvision import models\n",
      "\n",
      "# Load class indices\n",
      "with open('model/class_indices.json', 'r') as f:\n",
      "    class_info = json.load(f)\n",
      "    class_names = class_info['class_names']\n",
      "\n",
      "# Load PyTorch model\n",
      "checkpoint = torch.load('model/efficientnet_model.pth', map_location='cpu')\n",
      "model = EfficientNetClassifier(num_classes=len(class_names))\n",
      "model.load_state_dict(checkpoint['model_state_dict'])\n",
      "model.eval()\n",
      "\n",
      "print(f\"‚úÖ Model loaded: {len(class_names)} classes\")\n",
      "print(f\"   Classes: {class_names}\")\n",
      "\n",
      "============================================================\n",
      "\n",
      "‚úÖ Instructions saved to: deployment_instructions.py\n"
     ]
    }
   ],
   "source": [
    "def generate_deployment_code(bucket_name, gcs_prefix, signed_urls):\n",
    "    \"\"\"Generate code snippet for downloading models in deployment\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã MODEL DEPLOYMENT INSTRUCTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    code = f'''\n",
    "# ============================================================\n",
    "# KYC/AML Identity Classifier - Model Deployment\n",
    "# ============================================================\n",
    "\n",
    "# Method 1: Download using Signed URLs (No Authentication Required)\n",
    "# Valid for 365 days from generation date\n",
    "\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Create model directory\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "# Download model files\n",
    "model_urls = {{\n",
    "'''\n",
    "    \n",
    "    for file_name, url in signed_urls.items():\n",
    "        code += f'    \"{file_name}\": \"{url}\",\\n'\n",
    "    \n",
    "    code += f'''}}\n",
    "\n",
    "for file_name, url in model_urls.items():\n",
    "    print(f\"Downloading {{file_name}}...\")\n",
    "    urllib.request.urlretrieve(url, f\"model/{{file_name}}\")\n",
    "    print(f\"‚úÖ Downloaded {{file_name}}\")\n",
    "\n",
    "print(\"\\n‚úÖ All model files downloaded!\")\n",
    "\n",
    "# ============================================================\n",
    "# Method 2: Download using GCS API (Requires Authentication)\n",
    "# ============================================================\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "\n",
    "# Set credentials (if not running on GCP)\n",
    "# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/service-account.json'\n",
    "\n",
    "# Initialize client\n",
    "client = storage.Client(project='{PROJECT_ID}')\n",
    "bucket = client.bucket('{bucket_name}')\n",
    "\n",
    "# Download model files\n",
    "model_files = {MODEL_FILES}\n",
    "gcs_prefix = '{gcs_prefix}'\n",
    "\n",
    "os.makedirs('model', exist_ok=True)\n",
    "\n",
    "for file_name in model_files:\n",
    "    blob_path = f\"{{gcs_prefix}}/{{file_name}}\"\n",
    "    local_path = f\"model/{{file_name}}\"\n",
    "    \n",
    "    print(f\"Downloading {{file_name}}...\")\n",
    "    blob = bucket.blob(blob_path)\n",
    "    blob.download_to_filename(local_path)\n",
    "    print(f\"‚úÖ Downloaded {{file_name}}\")\n",
    "\n",
    "print(\"\\n‚úÖ All model files downloaded!\")\n",
    "\n",
    "# ============================================================\n",
    "# Method 3: Docker Deployment (Download during build)\n",
    "# ============================================================\n",
    "\n",
    "# Add to Dockerfile:\n",
    "'''\n",
    "    \n",
    "    code += '''\n",
    "# Download models during Docker build\n",
    "RUN mkdir -p /app/model && \\\\'''\n",
    "    \n",
    "    for i, (file_name, url) in enumerate(signed_urls.items()):\n",
    "        if i < len(signed_urls) - 1:\n",
    "            code += f'''\n",
    "    wget -O /app/model/{file_name} '{url}' && \\\\'''\n",
    "        else:\n",
    "            code += f'''\n",
    "    wget -O /app/model/{file_name} '{url}'\n",
    "'''\n",
    "    \n",
    "    code += f'''\n",
    "# ============================================================\n",
    "# Method 4: Load Model in Inference Code\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from torchvision import models\n",
    "\n",
    "# Load class indices\n",
    "with open('model/class_indices.json', 'r') as f:\n",
    "    class_info = json.load(f)\n",
    "    class_names = class_info['class_names']\n",
    "\n",
    "# Load PyTorch model\n",
    "checkpoint = torch.load('model/efficientnet_model.pth', map_location='cpu')\n",
    "model = EfficientNetClassifier(num_classes=len(class_names))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {{len(class_names)}} classes\")\n",
    "print(f\"   Classes: {{class_names}}\")\n",
    "'''\n",
    "    \n",
    "    print(code)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save to file\n",
    "    instructions_file = \"deployment_instructions.py\"\n",
    "    with open(instructions_file, 'w') as f:\n",
    "        f.write(code)\n",
    "    print(f\"\\n‚úÖ Instructions saved to: {instructions_file}\")\n",
    "\n",
    "generate_deployment_code(BUCKET_NAME, GCS_MODEL_PREFIX, signed_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c6e7d",
   "metadata": {},
   "source": [
    "## Step 10: List All Files in Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f452d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÅ Bucket contents:\n",
      "============================================================\n",
      "  identity_classification/v1/class_indices.json (0.00 MB)\n",
      "  identity_classification/v1/efficientnet_model.pth (50.12 MB)\n",
      "  identity_classification/v1/training_history.json (0.00 MB)\n",
      "============================================================\n",
      "Total: 3 file(s), 50.12 MB\n",
      "  identity_classification/v1/class_indices.json (0.00 MB)\n",
      "  identity_classification/v1/efficientnet_model.pth (50.12 MB)\n",
      "  identity_classification/v1/training_history.json (0.00 MB)\n",
      "============================================================\n",
      "Total: 3 file(s), 50.12 MB\n"
     ]
    }
   ],
   "source": [
    "def list_bucket_contents(bucket, prefix=None):\n",
    "    \"\"\"List all files in the bucket\"\"\"\n",
    "    print(\"\\nüìÅ Bucket contents:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "    total_size = 0\n",
    "    count = 0\n",
    "    \n",
    "    for blob in blobs:\n",
    "        size_mb = blob.size / (1024 * 1024)\n",
    "        total_size += blob.size\n",
    "        count += 1\n",
    "        print(f\"  {blob.name} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total: {count} file(s), {total_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "list_bucket_contents(bucket, prefix=GCS_MODEL_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc0cec",
   "metadata": {},
   "source": [
    "## Step 11: Model Versioning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86602672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Available Model Versions:\n",
      "============================================================\n",
      "  v1\n",
      "  v1\n",
      "============================================================\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def list_model_versions(bucket, base_prefix=\"document_classification\"):\n",
    "    \"\"\"List all available model versions\"\"\"\n",
    "    print(\"\\nüìã Available Model Versions:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    blobs = bucket.list_blobs(prefix=base_prefix)\n",
    "    versions = set()\n",
    "    \n",
    "    for blob in blobs:\n",
    "        # Extract version from path (e.g., document_classification/v1/file.pth -> v1)\n",
    "        parts = blob.name.split('/')\n",
    "        if len(parts) >= 2:\n",
    "            versions.add(parts[1])\n",
    "    \n",
    "    for version in sorted(versions):\n",
    "        print(f\"  {version}\")\n",
    "        version_blobs = bucket.list_blobs(prefix=f\"{base_prefix}/{version}\")\n",
    "        for blob in version_blobs:\n",
    "            size_mb = blob.size / (1024 * 1024)\n",
    "            print(f\"    - {os.path.basename(blob.name)} ({size_mb:.2f} MB)\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "list_model_versions(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef71c2e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **Trained model uploaded to Google Cloud Storage!**\n",
    "\n",
    "**What we did:**\n",
    "1. ‚úì Verified local model files\n",
    "2. ‚úì Authenticated with Google Cloud\n",
    "3. ‚úì Uploaded all model files to GCS\n",
    "4. ‚úì Generated signed URLs for public download\n",
    "5. ‚úì Created deployment instructions\n",
    "\n",
    "**Model Location:**\n",
    "```\n",
    "gs://kyc-aml-model/document_classification/v1/\n",
    "‚îú‚îÄ‚îÄ efficientnet_model.pth (PyTorch checkpoint)\n",
    "‚îú‚îÄ‚îÄ efficientnet_model.onnx (ONNX format - optional)\n",
    "‚îú‚îÄ‚îÄ class_indices.json (class mappings)\n",
    "‚îî‚îÄ‚îÄ training_history.json (training metrics)\n",
    "```\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "### 1. Update Inference API\n",
    "Update `api/main.py` to download model from GCS:\n",
    "\n",
    "```python\n",
    "from google.cloud import storage\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Download model on startup\n",
    "def download_model_from_gcs():\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('kyc-aml-model')\n",
    "    \n",
    "    files = ['efficientnet_model.pth', 'class_indices.json']\n",
    "    for file in files:\n",
    "        blob = bucket.blob(f'document_classification/v1/{file}')\n",
    "        blob.download_to_filename(f'model/{file}')\n",
    "```\n",
    "\n",
    "### 2. Docker Deployment\n",
    "Use signed URLs in Dockerfile to download models during build:\n",
    "\n",
    "```dockerfile\n",
    "# Download models from GCS\n",
    "RUN mkdir -p /app/model && \\\\\n",
    "    wget -O /app/model/efficientnet_model.pth 'SIGNED_URL_1' && \\\\\n",
    "    wget -O /app/model/class_indices.json 'SIGNED_URL_2'\n",
    "```\n",
    "\n",
    "### 3. Cloud Run / App Engine\n",
    "Models will be downloaded automatically on container startup using GCS API.\n",
    "\n",
    "### 4. Model Versioning\n",
    "To deploy a new version:\n",
    "```python\n",
    "# Upload to new version path\n",
    "GCS_MODEL_PREFIX = \"document_classification/v2\"  # New version\n",
    "# Re-run upload steps\n",
    "```\n",
    "\n",
    "### Managing Costs:\n",
    "- Standard storage: ~$0.02 per GB/month\n",
    "- Network egress: ~$0.12 per GB\n",
    "- For typical model (~50-100 MB): Less than $1/month\n",
    "\n",
    "### Useful Commands:\n",
    "```bash\n",
    "# List all models\n",
    "gsutil ls gs://kyc-aml-model/\n",
    "\n",
    "# Download specific model\n",
    "gsutil cp gs://kyc-aml-model/document_classification/v1/efficientnet_model.pth .\n",
    "\n",
    "# Download all model files\n",
    "gsutil -m cp gs://kyc-aml-model/document_classification/v1/* model/\n",
    "\n",
    "# Delete old version (if needed)\n",
    "gsutil -m rm gs://kyc-aml-model/document_classification/v1/*\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyc-aml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
